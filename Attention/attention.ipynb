{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFejFmVeoqBR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4As8TRJo7BK"
      },
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\" Scaled Dot-Product Attention \"\"\"\n",
        "\n",
        "    def __init__(self, scale):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        u = torch.bmm(q, k.transpose(1, 2)) # 1.Matmul\n",
        "        u = u / self.scale # 2.Scale\n",
        "\n",
        "        if mask is not None:\n",
        "            u = u.masked_fill(mask, -np.inf) # 3.Mask\n",
        "\n",
        "        attn = self.softmax(u) # 4.Softmax\n",
        "        output = torch.bmm(attn, v) # 5.Output\n",
        "\n",
        "        return attn, output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E13AV31uo-Rv"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-Head Attention \"\"\"\n",
        "\n",
        "    def __init__(self, n_head, d_k_, d_v_, d_k, d_v, d_o):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "        self.fc_q = nn.Linear(d_k_, n_head * d_k)\n",
        "        self.fc_k = nn.Linear(d_k_, n_head * d_k)\n",
        "        self.fc_v = nn.Linear(d_v_, n_head * d_v)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
        "\n",
        "        self.fc_o = nn.Linear(n_head * d_v, d_o)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v\n",
        "\n",
        "        batch, n_q, d_q_ = q.size()\n",
        "        batch, n_k, d_k_ = k.size()\n",
        "        batch, n_v, d_v_ = v.size()\n",
        "\n",
        "        q = self.fc_q(q) # 1.单头变多头\n",
        "        k = self.fc_k(k)\n",
        "        v = self.fc_v(v)\n",
        "        q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)\n",
        "        k = k.view(batch, n_k, n_head, d_k).permute(2, 0, 1, 3).contiguous().view(-1, n_k, d_k)\n",
        "        v = v.view(batch, n_v, n_head, d_v).permute(2, 0, 1, 3).contiguous().view(-1, n_v, d_v)\n",
        "\n",
        "        if mask is not None:\n",
        "          mask = mask.repeat(n_head, 1, 1)\n",
        "        attn, output = self.attention(q, k, v, mask=mask) # 2.当成单头注意力求输出\n",
        "\n",
        "        output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1) # 3.Concat\n",
        "        output = self.fc_o(output) # 4.仿射变换得到最终输出\n",
        "\n",
        "        return attn, output"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItbroxnhpChB"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\" Self-Attention \"\"\"\n",
        "    \n",
        "    def __init__(self, n_head, d_k, d_v, d_x, d_o):\n",
        "        super().__init__()\n",
        "        self.wq = nn.Parameter(torch.Tensor(d_x, d_k))\n",
        "        self.wk = nn.Parameter(torch.Tensor(d_x, d_k))\n",
        "        self.wv = nn.Parameter(torch.Tensor(d_x, d_v))\n",
        "\n",
        "        self.mha = MultiHeadAttention(n_head=n_head, d_k_=d_k, d_v_=d_v, d_k=d_k, d_v=d_v, d_o=d_o)\n",
        "\n",
        "        self.init_parameters()\n",
        "\n",
        "    def init_parameters(self):\n",
        "        for param in self.parameters():\n",
        "            stdv = 1. / np.power(param.size(-1), 0.5)\n",
        "            param.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        q = torch.matmul(x, self.wq)   \n",
        "        k = torch.matmul(x, self.wk)\n",
        "        v = torch.matmul(x, self.wv)\n",
        "\n",
        "        attn, output = self.mha(q, k, v, mask=mask)\n",
        "\n",
        "        return attn, output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3IFeTxIqc1Y"
      },
      "source": [
        "batch = 5"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76fVRCbmqUXz",
        "outputId": "d7ac3658-a943-48cb-8e79-8f470bab4cad"
      },
      "source": [
        "# SHA\n",
        "n_q, n_k, n_v = 2, 4, 4\n",
        "d_q, d_k, d_v = 128, 128, 64\n",
        "    \n",
        "q = torch.randn(batch, n_q, d_q)\n",
        "k = torch.randn(batch, n_k, d_k)\n",
        "v = torch.randn(batch, n_v, d_v)    \n",
        "mask = torch.zeros(batch, n_q, n_k).bool()\n",
        "\n",
        "attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
        "attn, output = attention(q, k, v, mask=mask)\n",
        "\n",
        "print(attn.size())\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 4])\n",
            "torch.Size([5, 2, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFs4klDKsxgQ",
        "outputId": "f3aa633a-cb8d-4475-e882-d8e09227b6b5"
      },
      "source": [
        "# MHA\n",
        "n_q, n_k, n_v = 2, 4, 4\n",
        "d_q_, d_k_, d_v_ = 128, 128, 64\n",
        "    \n",
        "q = torch.randn(batch, n_q, d_q_)\n",
        "k = torch.randn(batch, n_k, d_k_)\n",
        "v = torch.randn(batch, n_v, d_v_)    \n",
        "mask = torch.zeros(batch, n_q, n_k).bool()\n",
        "    \n",
        "mha = MultiHeadAttention(n_head=8, d_k_=128, d_v_=64, d_k=256, d_v=128, d_o=128)\n",
        "attn, output = mha(q, k, v, mask=mask)\n",
        "\n",
        "print(attn.size())\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([40, 2, 4])\n",
            "torch.Size([5, 2, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuXea4ecuoFS",
        "outputId": "a6d8eed8-2f07-4f29-97d6-4cca892270d5"
      },
      "source": [
        "# self-attention\n",
        "n_x = 4\n",
        "d_x = 80\n",
        "    \n",
        "x = torch.randn(batch, n_x, d_x)\n",
        "mask = torch.zeros(batch, n_x, n_x).bool()\n",
        "\n",
        "selfattn = SelfAttention(n_head=8, d_k=128, d_v=64, d_x=80, d_o=80)\n",
        "attn, output = selfattn(x, mask=mask)\n",
        "\n",
        "print(attn.size())\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([40, 4, 4])\n",
            "torch.Size([5, 4, 80])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD1hRClKyXRC",
        "outputId": "6b2f640b-ef2f-4307-d536-c45b6f923e2f"
      },
      "source": [
        "# self-attention\n",
        "n_x = 4\n",
        "d_x = 80\n",
        "    \n",
        "x = torch.randn(batch, n_x, d_x)\n",
        "mask = torch.zeros(batch, n_x, n_x).bool()\n",
        "    \n",
        "mha = MultiHeadAttention(n_head=8, d_k_=80, d_v_=80, d_k=128, d_v=64, d_o=80)\n",
        "attn, output = mha(x, x, x, mask=mask)\n",
        "\n",
        "print(attn.size())\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([40, 4, 4])\n",
            "torch.Size([5, 4, 80])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foifGv3x2Bv1"
      },
      "source": [
        "class SimpleSelfAttention(nn.Module):\n",
        "    \"\"\" Simple Self-Attention \"\"\"\n",
        "\n",
        "    def __init__(self, n_head, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_model #// n_head\n",
        "        self.d_v = d_model #// n_head\n",
        "\n",
        "        self.fc_q = nn.Linear(d_model, n_head * self.d_k)\n",
        "        self.fc_k = nn.Linear(d_model, n_head * self.d_k)\n",
        "        self.fc_v = nn.Linear(d_model, n_head * self.d_v)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(scale=np.power(self.d_k, 0.5))\n",
        "\n",
        "        self.fc_o = nn.Linear(n_head * self.d_v, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v\n",
        "\n",
        "        batch, n_x, d_model = x.size()\n",
        "        n_q = n_k = n_v = n_x\n",
        "\n",
        "        q = self.fc_q(x) # 1.单头变多头\n",
        "        k = self.fc_k(x)\n",
        "        v = self.fc_v(x)\n",
        "        q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)\n",
        "        k = k.view(batch, n_k, n_head, d_k).permute(2, 0, 1, 3).contiguous().view(-1, n_k, d_k)\n",
        "        v = v.view(batch, n_v, n_head, d_v).permute(2, 0, 1, 3).contiguous().view(-1, n_v, d_v)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.repeat(n_head, 1, 1)\n",
        "        attn, output = self.attention(q, k, v, mask=mask) # 2.当成单头注意力求输出\n",
        "\n",
        "        output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1) # 3.Concat\n",
        "        output = self.fc_o(output) # 4.仿射变换得到最终输出\n",
        "\n",
        "        return attn, output"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHusyBOR2E8Z",
        "outputId": "eda2f0e4-8f7b-4300-a184-9bebdd1dc5ad"
      },
      "source": [
        "# self-attention\n",
        "n_x = 4\n",
        "d_x = 81\n",
        "    \n",
        "x = torch.randn(batch, n_x, d_x)\n",
        "mask = torch.zeros(batch, n_x).bool() # (batch, n_k)\n",
        "mask = mask.unsqueeze(1).expand(-1, n_x, -1) # (batch, n_x, n_x), (batch, n_q, n_k)\n",
        "\n",
        "selfattn = SimpleSelfAttention(n_head=8, d_model=d_x)\n",
        "attn, output = selfattn(x, mask=mask)\n",
        "\n",
        "print(attn.size())\n",
        "print(output.size())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([40, 4, 4])\n",
            "torch.Size([5, 4, 81])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}